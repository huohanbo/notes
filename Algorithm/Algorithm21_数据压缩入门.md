# 参考资料
> [数据压缩入门 [美]柯尔特·麦克安利斯（Colt McAnlis），[美]亚历克斯·海奇（AleksHaecky） 2020-04](https://weread.qq.com/web/reader/52b32db071e2194152b5544kecc32f3013eccbc87e4b62e)

# 数据压缩简介
## 数据压缩算法
数据压缩算法有5类：
+ 变长编码（variable-length codes，VLC）
+ 统计压缩（statistical compression）
+ 字典编码（dictionary encodings）
+ 上下文模型（context modeling）
+ 多上下文模型（multi context modeling）

这几类算法也可以混合使用，因为其中有些算法的目的就是转换数据，使得其他的算法在压缩时更有效。

## 数据压缩原理
对数据进行压缩，通常有两个思路：
+ 减少数据中不同符号的数量（即让“字母表”尽可能小）。
+ 用更少的位数对更常见的符号进行编码（即最常见的“字母”所用的位数最少）。

数据压缩中的每一个算法都聚焦于解决这两件事情中的一件：
+ 要么通过打乱符号或减少符号的数量，将数据转换得更便于压缩。
+ 要么利用其中一些符号比其他符号更常见的事实，通过用最少的位数编码最常见的符号，实现压缩的目的。

进行实际的数据压缩时，需要综合考虑以下些因素：
+ 不同数据的处理方法不同。比如压缩一本书中的文字和压缩浮点型的数，其对应的算法就大不相同。
+ 有些数据必须经过转换才能变得更容易压缩。
+ 数据可能是偏态的。例如，夏天的整体气温偏高，也就是说高气温出现的频率比接近零度的气温出现的频率高很多。

## 二进制
数据压缩所做的无非就是尽可能减少表示特定数据集时所需的二进制位数量。

## 信息论
根据信息论的观点，一个数值所包含的信息内容等于，为了在一个集合中唯一地确定这个数值，需要做出的二选一（是/否）决定的次数。

## 熵
**熵：表示一个数所需要的最少二进制位数。**

## 突破熵
数据压缩领域的最前沿都是关于如何改变熵的。

突破熵的关键在于，通过利用数据集的结构信息将其转换为一种新的表示形式，而这种新表示形式的熵比源信息的熵小。

# 变长编码 VLC
**对于给定一个数据集中的符号，将最短的编码分配给最可能出现的符号。**

## 静态VLC

## 摩尔斯码
摩尔斯码为英语字母表中的每一个字符都分配了或长或短的脉冲，一个字母用得越频繁，其编码也就越短、越简单。
即使是追溯到19世纪，这也是对符号分配变长编码（variable-length codes，VLC）的最初实现之一，其目的则在于减少传输信息过程中所需要的总工作量。

## VLC运用
一般来说，对数据进行VLC通常有3个步骤。
(1) 遍历数据集中的所有符号并计算每个符号的出现概率。
(2) 根据概率为每个符号分配码字，一个符号出现的概率越大，所分配的码字就越短。
(3) 再次遍历数据集，对每一个符号进行编码，并将对应的码字输出到压缩后的数据流中。

+ 1.计算符号的概率
+ 2.为字符分配码字
+ 3.编码
+ 4.解码

# 统计编码
统计编码（statistical encoders）算法。
这类算法无须将特定的整数转换为特定的码字，而是通过数据集中符号的出现概率来确定新的、唯一的变长码字。
最终的结果就是，给定任何输入数据，我们都能为其构造出一套自定义的码字集，而无须去匹配现有的VLC方法。
该算法以数据流中符号的频率为依据，为该数据流中的各个符号分配长度可变的码字，从而使最终的输出压缩得更小。

## 哈夫曼编码
哈夫曼编码可能是生成自定义VLC最直接、最有名的方法。给定一组符号及其出现频率，该方法能生成一组符号平均长度最短的VLC。

它的工作原理：**将数据排序后建立决策树（decision tree），然后从“树干”一直往下直到“树叶”为止，并记录下所做的是/否选择（几乎又回到了“20个问题”这个游戏上）。**这里所说的“树叶”就是我们要找的各个符号。

## 算术编码
哈夫曼编码简单、高效，也能为单个的数据符号生成最佳的码字。
然而，对于给定的符号集来说，它并非总是生成最有效的码字。
事实上，哈夫曼编码能生成理想VLC（即码字的平均长度等于符号集的熵）的唯一情形是，各个符号的出现概率等于2的负整数次幂（即是1/2、1/4或1/8这样的值）。

与按照1∶1的比例为每个符号分配一个码字不同，算术编码算法会将整个输入流转换为一个长度很长的数值，而它的lb表示则与整个输入流真正的熵值很接近。
算术压缩的神奇之处在于，它将转换应用到整个源数据上以生成一个输出值，而表示这个输出值所需要的二进制位数比源数据本身少。

算术编码的工作原理：**将字符串转换为一个数，与字符串相比，表示这个数需要的二进制位数要少一些。**

## 非对称数字系统 ANS
非对称数字系统（asymmetric numeral systems，ANS）。
实际上，ANS是一种新的精确熵编码方法，所得到的结果可以和最优熵任意接近，它的压缩率与算术编码接近，而性能则与哈夫曼编码相当。

ANS虽然在数据压缩领域里出现的时间还不长，但是已开始取代过去20多年里占据主流地位的哈夫曼编码和算术编码。

这一算法又出现了一个被称为有限状态熵（Finite State Entropy，FSE）的更注重性能的版本，它只使用加法、掩码和移位运算。

# 自适应统计编码
数据压缩领域内一个重要的理论，即局部性很重要（localitymatters）。
由于数据流一般以线性的方式生成，因此数据流中很有可能会出现某一部分的特征与其他部分完全不同的情况。

在编码时，如果“期望”的熵与“实际的符号平均二进制位数”之间出现显著差异，那么统计编码算法会重置概率表，并使用重置后的概率表进行编码。
这种具有适应数据流熵的局部特性能力的统计编码算法，通常被称为**“动态”或“自适应”统计编码算法**。

# 字典转换
## 基本字典转换
统计压缩主要关注数据流中单个符号的出现概率，这一概率与其周围可能出现的符号无关。
但这忽略了真实数据的基本属性：上下文及词语的组合，或者简单地说就是短语。
如果考虑的对象不再是单个的符号，而是一组相邻的符号，我们就走出了统计压缩的世界，来到了字典转换的世界。

字典转换的工作方式：**给定源数据流，首先构建出单词字典（而不是符号字典），然后再将统计压缩应用到字典中的单词上。**

字典转换并非是要去替代统计编码，相反，它只是你先应用到数据流上的一个转换，这样统计编码算法就能更有效地对其编码。
因此，字典转换实际是一个数据流的预处理阶段，经过这样的预处理后，生成的数据集会更小，比源数据流压缩率更高。

## LZ算法
1977年，两位研究人员Abraham Lempel和Jacob Ziv提出了几种解决“理想分词”问题的方法。
这些算法根据提出的年份分别被命名为LZ77 和LZ78。
它们在**找出最佳分词**方面非常高效，30多年来还没有其他算法可以取代它们。

### LZ算法的工作原理
LZ算法尝试通过在读取的字符串中寻找当前单词的匹配来分词。
与读取一组符号然后向后查找它是否重复出现不同，LZ算法向前查找当前单词是否出现过。

这样做会对编码过程产生如下两个重要影响：
+ 在数据流的前半部分，由于我们见过的单词很少，因此出现新单词的可能性很大；而在数据流的后半部分，由于已经有了很大的缓冲区，因此出现匹配的可能性更大。
+ 向前寻找匹配可以让我们找出“最长的匹配词”。

# 上下文数据转换
统计编码算法工作时会为每个符号分配一个长度可变的码字，压缩主要来自于为越频繁出现的符号分配越短的码字。
而字典转换的分词过程会识别出数据集中最长且最频繁出现的那些符号。

除了字典变换之外，还有一整套其他的变换都是按照同样的原理工作的：给定一组相邻的符号集，对它们进行某种方式的变换使其更容易压缩。
我们通常称这样的变换为“上下文变换”（contextual transform）。

变换数据的方法有很多种，但其中有3种对现代的数据压缩来说最为重要：
+ 行程编码(run-length encoding，RLE）
+ 增量编码（delta coding）
+ 伯罗斯–惠勒变换（Burrows-Wheeler transform，BWT）

## 行程编码 RLE
RLE主要针对的是连续出现的相同符号聚类的现象，它会用包含符号值及其重复出现次数的元组，来替换某个符号一段连续的“行程”（run）。

例如：字符串AAAABBBBBBBBCCCCCCCC就可以编码为[A,4][B,8][C,8]。

RLE算法最适用于大多数符号都连续重复出现的数据集。如果要处理的数据集没有这样的性质，那么RLE算法并不适用。

压缩RLE算法输出的结果需要一些技巧：
首先，需要将数据集分成两部分：字面值流和行程长度流。
对字面值流，可以根据自己的意愿选择一种编码器，剩下的行程长度流才是压缩问题的真正所在。

## 增量编码
增量编码，其实就是将一组数据转换为各个相邻数据之间的相对差值（即增量）的过程。
在数值型数据这样普遍而其熵值又如此偏高的情况下，增量编码提供了一种不依靠统计的转换。

以下面这组数为例：
```
[1,3,6,8,10]
```
从第二个数开始，用当前数减去前一个数，得到了增量编码之后的数据：
```
[1,3–1,6–3,8–6,10–8] → [1,2,3,2,2]
```

一般来说，增量编码的目的就是缩小数据集的变化范围。更确切地说，是为了减少表示数据集中的每个值所需要的二进制位数。

## 前移编码 MTF
上下文数据转换背后的基本思想是，数据的排列次序中包含着一些有助于编码未来符号的信息，前移编码（move-to-front coding，MTF）利用的也是这样的信息。

**与RLE和字典编码器只考虑直接相邻的符号不同，MTF考虑更多的是在较短的窗口内某个特定符号的出现次数。**

### MTF的工作原理
它是通过另外管理一组数据来工作的，其中包含的是数据集中所有不同的值，我们称为SortedArray。
当从数据流中读取一个值时，我们会找出该值在SortedArray中的索引并将此索引值输出，然后更新SortedArray，将该值移到最前面，即让其索引变为0。

### 压缩MTF后的数据
MTF生成的输出流的熵通常要比源数据流小，这让它的输出成为传递给统计压缩器进行进一步压缩的首选对象。
由于MTF生成的输出流中有很多的0和1，因此简单的统计编码算法就可以工作得很好。

## 块排序变换 BWT
BWT的工作原理：
通过打乱数据流次序来让重复的子串聚集在一起。
这一操作本身不能压缩数据，却可以为后续的压缩系统提供转换好的数据流，方便压缩。

BWT会打乱数据流中符号的顺序，并试图让相同的符号簇彼此靠近，这一行为通常称为**字典序排列（lexicographical permutation）**。

熵作为度量单位，它的一个问题是没有考虑符号之间的顺序。通过转换数据流中符号之间的顺序，可以让数据流更容易压缩。
不过遗憾的是，纯粹的排序是单方向的。
也就是说，在对数据排序后，如果没有更多额外的信息指明它是如何变化的，我们无法让数据重新回到未排序的状态。

压缩BWT后的数据：
**最常见的算法是将BWT的输出作为MTF的输入，经过处理后接着用统计编码算法处理。**这基本上就是BZIP2的内部工作原理。

具体的实现：
我们通常将BWT称为**块排序变换**，具体实现时，它会将整个文件分为许多1MB大小的数据块，然后在每个数据块上分别应用该算法。

# 数据建模
多上下文编码算法背后的基本概念，它会考虑最后观察到的几个符号以确定当前符号的理想编码位数。

这类编码器也可以认为是统计压缩器的“加强”（on-steroids）版。
它将自适应模型和多种符号码字对应表结合起来，根据前面观察到的符号，为当前符号生成尽可能短的码字。

## 马尔可夫链 Markov chain
马尔可夫链（Markov chain）：马尔可夫链是一种离散的随机过程，其未来的状态只取决于现在，而与过去的历史无关。

也可以将它可视化为一棵树，这里每个节点代表一个活动，每次转移则有相应的概率。

马尔可夫链这一概念能很好地融入现有的模型，因为可以认为统计编码算法就是一阶马尔可夫链。

## 部分匹配预测算法 PPM
要使马尔可夫链算法变得实用，就必须要解决内存消耗问题与计算性能问题，即使用最佳链来编码。

John Cleary与和Ian Witten于1984年提出了马尔可夫链算法的一种具体实现，并称之为部分匹配预测算法（**prediction by partialmatching，PPM**），该算法在内存消耗与计算性能方面表现都还不错。

## 上下文混合算法
对PPM算法的改进，让我们有了新的数据压缩算法，PAQ 系列算法。
特别是在PPMZ算法中，对于符号如何去响应匹配，人们尝试了多种类型的上下文。

随着时间的推移，这一概念逐渐被称为上下文混合算法（context mixing），即为了找出给定符号的最佳编码，我们会使用两个或者更多的统计模型。

# 换个话题
当前压缩可以分为两类，即多媒体数据压缩（media-specific compression）与通用压缩（general purpose compression）。

## 多媒体数据压缩
有损数据转换的种类特别多，每一种都针对特定的多媒体文件（针对图像文件的就不太适用于音频文件）和内容类型（灰度图像与全彩图像使用的压缩算法同样不同）。

## 通用压缩
相比而言，通用压缩工具是设计用来压缩除多媒体数据以外的其他数据。
像DEFLATE、GZIP、BZIP2、LZMA和PAQ这些算法，都是将各种无损转换结合起来，用来压缩诸如文本、源代码、序列化数据以及二进制内容等其他不能使用有损压缩工具压缩的非多媒体文件。

标准的HTTP协议栈允许数据包使用GZIP和BZIP编码，现在又多了一种Brotli（前提是服务器端和客户端都支持），也就是说，网页、JavaScript文件、留言以及商店列表这些内容都会在解压后显示到你的设备上。

# 评价数据压缩
略

# 压缩图形数据
## 图像质量与文件大小
通常来说，图像压缩工具会提供一个整数参数，让你来决定图像的质量。**随着这个值变小，图像的大小也会变小，当然质量也会变差。**

这个值主要是用来控制有损算法为了压缩效果更好，而在转换数据时采取的力度。
更差的质量就意味着有更多的颜色被丢弃，或者是有更多的边缘信息被忽略，所有这些都是为随后的统计编码阶段生成更多的重复符号。

## 图像尺寸
在现今的移动世界，有很多屏幕尺寸不同、处理能力各异的设备。

在处理图像时应该使用什么样的分辨率？
更好的方法是直接在云端调整图像的大小，或者在某处缓存调整大小后的图像，这样就能向小屏幕发送小尺寸的图像。

发送合适大小的图像给用户有以下好处：
+ 发送的数据量更少了，这会更快，也会节省用户的套餐费用。
+ 可以节省用户的设备空间。
+ 无须再调整图像的大小。（如果要在 GPU中处理 4 MB大小的图像，而渲染的时候只是缩略图，是不是很浪费资源？）
+ 解码会更快，加载会更快，显示也会更快。

## 图像格式
在移动应用程序和网页开发中最常用的几种图像格式：**PNG、JPG、GIF和WebP**。

### PNG
便携式网络图像格式（Portable Network Graphics format，PNG），是一种无损图像格式，它使用GZIP这样的压缩工具使数据量变小。
由于是一种无损的图像格式，因此压缩后的图像质量与源图像相同。
这正是它的优点，既能保证图像的高质量又能压缩数据量，当然压缩的程度不像有损压缩那么大。
PNG格式支持alpha透明度。PNG格式支持文件中存在元数据块。

### JPG
如果你对透明度没有明确的需求，那么联合图像专家小组（Joint PhotographicExperts Group，JPEG或JPG）格式可以说是一个更好的选择。
JPG是一种用于摄影图像的格式，它不支持alpha透明度。
它包含一个功能强大的有损压缩工具，我们可以通过一个质量值来控制它，以达成对文件大小与图像质量的权衡取舍。

### GIF
GIF是另外一种支持透明度的格式，此外它还支持动画（这也是cats on theinternet thing的直接原因）。

### WebP
WebP格式为用户提供了介于PNG和JPG之间的中间地带。WebP既支持无损模式和透明度，同时也支持有损模式。

## GPU纹理格式
GPU能直接渲染的像素压缩格式是存在的，因此你可以利用这一点，将从网络中传输过来的数据解压为这些压缩格式之一，这样GPU就可以直接渲染，而无须解压这一步骤。
DXT、ETC和PVR就是几种这样的有损像素压缩格式。

## 矢量格式
矢量图像格式：一般来说，这样的格式里包含的是一些程序指令，只要按照顺序执行就会生成最终的输出图像。

矢量格式适用于标志、技术图纸以及简单的图像样式。
虽然它可以使需要传输的数据变小，但在渲染时由于需要重新生成图像，因而加大客户端的时间开销。

# 序列化数据
除了图像数据，序列化内容是网络应用程序处理第二多的数据格式。
	
序列化是将高级数据对象转化为二进制字符串的过程（与之相反的过程则称为反序列化）。

## 常见的使用场景
序列化数据最常见的使用场景：
+ 服务器动态生成的数据
+ 服务器拥有的静态数据
+ 服务器拥有的静态数据
+ 客户端拥有的静态数据

## 序列化格式的问题
**两种最常见的序列化格式就是JSON和XML。**
虽然易用并且很流行，但是这两种格式还是遇到了一些非常特殊的压缩问题。

### 可读文本信息冗余
JSON和XML这两种格式最吸引人的地方在于，它们（或多或少）是可读的。

其优点在于格式十分灵活，但缺点也很明显，为了保证可读性，其中包含了大量的冗余信息。

### 解码时间长
需要注意的是，这样的文本格式在解码时常常会需要很长时间。
其原因是多方面的：
+ 一是字符串的输入必须经过强力操作才能转化为内存对象（例如将ASCII符号转换为整数就不那么容易）
+ 二是在加载期间将数据保存在临时内存里并非总是高效的
+ 三是对旧格式的兼容也会使得编码和解码变慢

## 更小的序列化数据
### 使用二进制序列化格式
最简单有效的方法就是将XML和JSON格式扔到一边，找出一种**二进制序列化格式**来代替它们。
例如：Protobufs、Flatbuffers和Cap'n Proto。

虽然二进制格式不再具备XML和JSON的可读性，但是能保证数据用紧凑和高效的二进制形式编码。这样，文件就会变小，加载速度也会变快。

### 重构列表以获得更好的压缩

### 组织数据以便高效获取

### 将数据切分为适当的压缩格式

# 有损数据压缩
无

# 让世界变得更小
略